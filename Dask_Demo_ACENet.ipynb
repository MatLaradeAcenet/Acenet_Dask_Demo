{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the Python DASK library\n",
    "\n",
    "Mat Larade -- mat.larade@ace-net.ca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note about the slides ...\n",
    "\n",
    "They are in a Jupyter notebook.\n",
    "\n",
    "https://github.com/MatACENET/Dask-Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other resources\n",
    "\n",
    "I won't be doing a comprehensive overview Dask, but\n",
    "here is some supplimental material:\n",
    "* Sharcnet webinar by Jinhui Qin\n",
    "  * Great overview of Dask\n",
    "  * https://youtube.sharcnet.ca\n",
    "* U of A Workshop by Christ Want\n",
    "  * https://ualberta-rcg.github.io/python-dask/\n",
    "* Google for `dask tutorial`\n",
    "  * Lots of good notebooks in a Github repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python - General\n",
    "\n",
    "* Python is an interperted software language that has a great deal of support in scientific and mathematical computing\n",
    "\n",
    "* The major packages we will be referencing are Numpy, Pandas and of course Dask\n",
    "* Variables are assigned using \n",
    "\n",
    "        variable_name = variable_content\n",
    "     Variables may not start with a number\n",
    "     \n",
    "     May only contain underscores and alphanumeric characters\n",
    "     \n",
    "     Are case-sensitive\n",
    "     \n",
    "     \n",
    "* Lists are created in python using \n",
    "\n",
    "        list_name = [var_1, var_2, ... var_n]\n",
    "\n",
    "* Functions are created using \n",
    "\n",
    "        def function_name(arg_1, arg_2, ... arg_n):\n",
    "            code_goes_here\n",
    "\n",
    "* Functions are called using \n",
    "    \n",
    "        function_name(arg_1, arg_2, ... arg_n)\n",
    "        \n",
    "* For loops are created using\n",
    "\n",
    "        for object in iterator:\n",
    "            code_goes_here\n",
    "            \n",
    "* Loop and function scope is indicated by indentation depth\n",
    "* Colons are used to indicate a for loop or function has been defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i in range(0,5):\n",
    "    print(i*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Functions and libraries are imported using\n",
    "\n",
    "        (1) import package_name**\n",
    "\n",
    "        (2) from package_name import sub_package\n",
    "\n",
    "* How functions from libraries are used:\n",
    "\n",
    "       (1) package_name.sub_package(args)\n",
    "       \n",
    "       (2) sub_package(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks - Usage\n",
    "\n",
    "* Jupyter notebooks, for those of you unfamiliar with python are kind of similar to debugging with VSCode, where you step through code in units at a time.\n",
    "* To execute code in a cell, press **[Shift + Enter]**.\n",
    "* Each unit is a cell, and all of the code is executed sequentially inside of the cell.\n",
    "* State is preserved between cells and cells can be run out of order or run multiple times.\n",
    "* Jupyter notebooks do a lot of work behind the scenes which may or may not be a benefit to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell will change value depending on how the above two cells are run.\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Notebooks - Warnings\n",
    "\n",
    "* I cannot reccomend using Jupyter notebooks outside of teaching because of the issues above, and they are slow to execute, heavyweight, and lack many of the features found inside of most IDE's, such as linting and convenient copying and pasting\n",
    "* If you want to learn or write python, I reccomend using:\n",
    "* **Visual Studio Code** - Full fat IDE with a built in python debugger, can easily be connected to anaconda or other sources of python - My preferred solution to Python on Linux and Windows - https://code.visualstudio.com/\n",
    "* **Sublime Text** - Text editor with many plugins to help with linting, code highlighting, etc - Useful for when a full IDE is overkill, such as disposable scripts. - https://www.sublimetext.com/\n",
    "* **Vim** - Doesn't really need an introduction, but is very useful when editing code on the command line and has a fairly rich feature set if you spend the time with it. Highly reccomended for interacting with code on the command line, such as our HPC clusters. - Literally every command line interface seems to come bundled with Vim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Python on our clusters\n",
    "\n",
    "* To do python on the clusters, we use a virtual environment.\n",
    "* To create and load into a virtual environment, enter these commands in your terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "module load python/3.7\n",
    "module load scipy-stack\n",
    "virtualenv --no-download ~/Parallel_Tutorial\n",
    "source ~/Parallel_Tutorial/bin/activate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* To deactivate the environment enter in \"deactivate\", and you should be back to the normal bash terminal.\n",
    "* Now that the environment exists, we can install some python packages using python's package manager, pip [Pip Installs Python]\n",
    "* There are several ways to do this, but the packages that we require are: Scikit-Learn, DASK, Graphviz, Numpy, Pandas and MatPlotLib\n",
    "* Scipy stack takes care of most of that for us, installing Numpy Pandas, and MatPlotLib. (thanks, sysadmins!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --no-index --upgrade pip\n",
    "pip install --no-index dask\n",
    "pip install --no-index scikit-learn\n",
    "pip install toolz\n",
    "pip install graphviz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DASK - General\n",
    "\n",
    "Dask is a lazy framework that automates parallel operations. Lazy, meaning that it doesn’t operate until it is told to.\n",
    "\n",
    "Dask is conceptually similar to a dishwasher\\*, where it will wait idle until it’s told to do everything all at once**.\n",
    "\n",
    "*Using Dask does not guarantee your code will be clean.\n",
    "\n",
    "**Not everything actually done all at once, Dask does several things at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An artist's rendition of DASK before running based on my description\n",
    "<left>![](./Pictures/Picture1.png)</left>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask - Distributed\n",
    "\n",
    "Dask can operate as it’s own task manager in one of three ways:\n",
    "1. Threaded – Using small, independent chunks of code running in the same instance of a computer program. Best for operations on numerical data that doesn’t keep a Global Interpreter Lock* (e.g. Numpy, Pandas, Scikit-Learn).\n",
    "2. Processes – Sends data to separate instances of a program to be processed.  Generally works best when there are several instances of a program running at simultaneously that hold the Global Interpreter Lock*.\n",
    "3. Single-Threaded – Does one chunk of code at a time, with no parallel capacity. Primarily for Debugging.\n",
    "\n",
    "Parallel programming with Python has a complicated history because of a design decision, the Global Interpreter Lock, which limits python to a single thread most of the time. Global interperter lock is a complicated subject that I'm not prepared to digress into. If you really want to know about the GIL, https://realpython.com/python-gil/ , https://en.wikipedia.org/wiki/Global_interpreter_lock\n",
    "\n",
    "TL,DR:  Numpy, Pandas, and Scikit-Learn work around the problem using threads and Dask can work with that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Where I am running locally, I begin by spawning a client.  There are several different ways of doing this, however, we will be using a 4-core, 1 worker cluster, with a memory limit of 2GB of memory (adjust the memory req for whatever you can spare if you need to).\n",
    "* This client will open up a \"Dashboard\" which you can use to monitor what's going on under the hood of your DASK instance.\n",
    "* If you'd like to read more about specific clients, and how they operate, please refer to https://distributed.dask.org/en/latest/ as these features are difficult to use effectively on HPC systems\n",
    "* We will not be delving deep into the distributed modules in Dask, as they are both very complex and do not work well with our HPC infrastructure, however, it is a powerful set of tools inside of the Dask kit, especially when operating on your local workstations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for me to create diagnostic reports, and your own if you review these notbooks \n",
    "# You do not need to include this in any of your scripts\n",
    "#from dask.distributed import Client, progress\n",
    "#client = Client(processes=False, threads_per_worker=4,\n",
    "#                n_workers=1, memory_limit='2GB')\n",
    "#client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask - Delayed\n",
    "\n",
    "* The Delayed command holds back the operations and assigns them to different cores.\n",
    "* Rather than waiting for tasks to finish sequentially, initial tasks are assigned to different cores that operate simultaneously. \n",
    "* When a core finishes it’s job, it gets a new operation, similar to customs in an Airport.\n",
    "\n",
    "We are going to demonstrate how dask.delayed works using an increment function that takes one second to execute, and and add function that takes one second to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from dask import delayed\n",
    "\n",
    "def increment(x):\n",
    "    sleep(1)\n",
    "    return x + 1\n",
    "\n",
    "def add(x, y):   \n",
    "    sleep(1)\n",
    "    return x +  y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes three seconds to run because we call each\n",
    "# function sequentially, one after the other\n",
    "\n",
    "x = increment(1)\n",
    "y = increment(2)\n",
    "z = add(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This runs immediately, all it does is build a graph\n",
    "\n",
    "x = delayed(increment)(1)\n",
    "y = delayed(increment)(2)\n",
    "z = delayed(add)(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This actually runs our computation using a local process pool\n",
    "\n",
    "z.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excercise\n",
    "\n",
    "Use Dask.Delayed to improve the execution speed of this for loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "output = []\n",
    "\n",
    "for i in range(100):\n",
    "    j = increment(i)\n",
    "    output.append(j)\n",
    "\n",
    "total = sum(output)\n",
    "    \n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
   "#YourCodeHere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "total.compute()",
    "total.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: When benchmarked with 1,000 and 10,000 samples of data on my hardware, the runtimes diverege as follows:\n",
    "\n",
    "Serial:       1,000: 16:40       10,000: 2:47:40\n",
    "\n",
    "Answer1:      1,000:  4:12       10,000: 0:42:27\n",
    "\n",
    "Answer2:      1,000:  4:11       10,000: 0:42:25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask - Bagging\n",
    "\n",
    "* Bagging works by taking semi-structured data and preparing that data to be operated on in parallel.\n",
    "* Bags work on what they have to at any given time, so once a data chunk is processed, it is removed from memory, thus reducing the overall footprint of operations.\n",
    "* Bagging tends to be less useful than the other methods we will discuss, as unstructured data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Portions up your semi-structured data so you can...\n",
    "![](./Pictures/Picture4.jpg)\n",
    "\n",
    "## You can run more data through faster than single threads\n",
    " \n",
    "![](./Pictures/Picture3.jpg)\n",
    "*in the metaphor, each core is it's own pizza oven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import bag\n",
    "bag1 = bag.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=2)\n",
    "bag1.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bag` objects hold the standard functional API found in projects like the Python standard library, `toolz`, or `pyspark`, including `map`, `filter`, `groupby`, etc..\n",
    "\n",
    "Operations on `Bag` objects create new bags.  Call the `.compute()` method to trigger execution, as we saw for `Delayed` objects.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_even(n):\n",
    "    return n % 2 == 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_list = []\n",
    "for i in range(0,10):\n",
    "    small_list.append(i)\n",
    "bag_small = bag.from_sequence(small_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_list_small = []\n",
    "for item in small_list:\n",
    "    if is_even(item):\n",
    "        new_list_small.append(item**2)\n",
    "print(new_list_small) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered_bag_small = bag_small.filter(is_even).map(lambda x: x ** 2)\n",
    "filtered_bag_small.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_bag_small.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_list = []\n",
    "for i in range(0,10000):\n",
    "    big_list.append(i)\n",
    "bag_big = bag.from_sequence(big_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new_list_big = []\n",
    "for item in big_list:\n",
    "    if is_even(item):\n",
    "        new_list_big.append(item**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "filtered_bag_big = bag_big.filter(is_even).map(lambda x: x ** 2)\n",
    "filtered_bag_big.compute()\n",
    "\n",
    "print(\"This statement is to stop the Jupyter from dumping the entire output out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "list_test = []\n",
    "for item in bag_big:\n",
    "    if is_even(item):\n",
    "        delayed(list_test.append(item**2))\n",
    "        \n",
    "print(list_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when the size of the list increases, bagging becomes a more efficent way of processing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy\n",
    "\n",
    "![](./Pictures/Math.jpg)\n",
    "* Numpy is a widely used and comprehensive mathematics package in python\n",
    "* Numpy stands for \"Numerical Python\", and handles tasks such as matrix math, trig, linear algebra, etc.\n",
    "* Is very useful for matrix math, and matrix-like operations, such as loading data into machine learning algorithms\n",
    "* This talk will not deep-dive into numpy, but will instead talk about a few key features, give a rough idea of what numpy is capable of, before moving on to dask arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Numpy primarily works on arrays of data, though several functions can be called on intigers or floats.\n",
    "* Arrays may have between 1 and n dimensions, and are primarily limited in size by the amount of ram a computer has to operarte on them.\n",
    "* We will walk through a quick example on how to create a numpy arrray, then pass it over to DASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.arange(16)\n",
    "print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_square = array.reshape(4,4)\n",
    "print(array_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(array.shape)\n",
    "print(array_square.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy \n",
    "* Numpy backs onto well-written, compiled software, so it performs faster and more reliable calculations than most code you can write yourself\n",
    "* Numpy has most of the mathematical functions that one could ask for, a non-exhaustive list includes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tan()\t\t\t\tCompute tangent element-wise.\n",
    "arcsin()\t\t\tInverse sine, element-wise.\n",
    "arccos()\t\t\tTrigonometric inverse cosine, element-wise.\n",
    "arctan()\t\t\tTrigonometric inverse tangent, element-wise.\n",
    "arctan2()\t\t\tElement-wise arc tangent of x1/x2 choosing the quadrant correctly.\n",
    "degrees()\t\t\tConvert angles from radians to degrees.\n",
    "rad2deg()\t\t\tConvert angles from radians to degrees.\n",
    "deg2rad\t\t\t\tConvert angles from degrees to radians.\n",
    "radians()\t\t\tConvert angles from degrees to radians.\n",
    "hypot()\t\t\t\tGiven the “legs” of a right triangle, return its hypotenuse.\n",
    "unwrap()\t\t\tUnwrap by changing deltas between values to 2*pi complement.\n",
    "rint()\t\t\t\tRound to nearest integer towards zero.\n",
    "fix()\t\t\t\tRound to nearest integer towards zero.\n",
    "floor()\t\t\t\tReturn the floor of the input, element-wise.\n",
    "ceil()\t\t\t\tReturn the ceiling of the input, element-wise.\n",
    "trunc()\t\t\t\tReturn the truncated value of the input, element-wise.\n",
    "expm1()\t\t\t\tCalculate exp(x) – 1 for all elements in the array.\n",
    "exp2()\t\t\t\tCalculate 2**p for all p in the input array.\n",
    "log10()\t\t\t\tReturn the base 10 logarithm of the input array, element-wise\n",
    "log2()\t\t\t\tBase-2 logarithm of x.\n",
    "log1p()\t\t\t\tReturn the natural logarithm of one plus the input array, element-wise.\n",
    "logaddexp()\t\t\tLogarithm of the sum of exponentiations of the inputs.\n",
    "logaddexp2()\t\tLogarithm of the sum of exponentiations of the inputs in base-2.\n",
    "convolve()\t\t\tReturns the discrete, linear convolution of two one-dimensional sequences.\n",
    "sqrt()\t\t\t\tReturn the non-negative square-root of an array, element-wise.\n",
    "square()\t\t\tReturn the element-wise square of the input.\n",
    "absolute()\t\t\tCalculate the absolute value element-wise.\n",
    "fabs()\t\t\t\tCompute the absolute values element-wise.\n",
    "sign()\t\t\t\tReturns an element-wise indication of the sign of a number.\n",
    "interp()\t\t\tOne-dimensional linear interpolation.\n",
    "maximum()\t\t\tElement-wise maximum of array elements.\n",
    "minimum()\t\t\tElement-wise minimum of array elements.\n",
    "real_if_close()\t\tIf complex input returns a real array if complex parts are close to zero.\n",
    "nan_to_num()\t\tReplace NaN with zero and infinity with large finite numbers.\n",
    "heaviside()\t\t\tCompute the Heaviside step function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(np.square(array))\n",
    "print(np.sqrt(array_square))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest reason to use numpy, however is that it allows your data to be used by various other frameworks, such as...\n",
    "![](./Pictures/Scikit_learn_logo.svg)\n",
    "![](./Pictures/TensorFlowLogo.png)\n",
    "![](./Pictures/Pytorch_logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask - Arrays\n",
    "\n",
    "* Dask arrays are a multidimensional, structured data format that are useful for manipulating large amounts of numerical data. \n",
    "* Dask arrays can be thought of as distributed Numpy arrays and can be manipulated in parallel.\n",
    "* Much of the syntax for Dask arrays is lifted directly from numpy.\n",
    "* Dask arrays are useful for pre-processing larger than memory datasets, and can also be used to feed some machine learning algorithms, just like numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask import array as da\n",
    "new_dask_array = da.from_array(array_square)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_square = da.square(new_dask_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dask_square.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_square.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "np.square(array_square)\n",
    "dask_square.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import a small dataset from SKLearn for later\n",
    "from sklearn import datasets\n",
    "iris_data = datasets.load_iris()\n",
    "iris_attributes = iris_data.data\n",
    "iris_classes = iris_data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_attributes_dask = da.from_array(iris_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = da.mean(iris_attributes_dask)\n",
    "da.mean(iris_attributes_dask).compute()\n",
    "mean.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "da.mean(iris_attributes_dask).visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(np.mean(iris_attributes))\n",
    "print(da.mean(iris_attributes_dask).compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "z = da.mean(x).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z.compute()\n",
    "z.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also force DASK arrays to hold the data in memory as well, using the persist comman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_persist = x.persist()\n",
    "z_persist = da.mean(x_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "z_persist.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask - Arrays: Limitations\n",
    "\n",
    "* Not all of numpy is included in dask, specifically sorting, listing, and some smaller functions\n",
    "* Dask tends to be slightly slower than loading the entire dataset into memory\n",
    "## Dask arrays allows you to move mountians\n",
    "![](./Pictures/Picture6.jpg)\n",
    "## A bucket of rocks at a time\n",
    "![](./Pictures/Picture5.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python - Pandas\n",
    "\n",
    "![](./Pictures/Panda.jpg)\n",
    "* Pandas is effectively a spreadsheeting program for python, running on Python/Cython/C with a Python front-end.\n",
    "* Pandas is similar to Excel, but does not have a GUI, so it is faster, but less user friendly.\n",
    "* Pandas tends to operate more quickly and efficiently than full-fat Microsoft Excel.\n",
    "* Pandas operates primarily using DataFrames, rougly equivalent to Excel Sheets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is the google search trends for the word \"parallel\"\n",
    "import pandas as pd\n",
    "\n",
    "pandas_dataframe = pd.read_csv('Data/multiTimeline.csv')\n",
    "print(pandas_dataframe.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas offers much of the same functionality as Excel, except you need to print out data to the terminal to see it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_dataframe.columns.values)\n",
    "print(pandas_dataframe.index.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can print out data from a specific locus or from a pair of headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_dataframe.iloc[5, 0])\n",
    "print(pandas_dataframe.loc['2019-07-07', 'Category: All categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a dump of the information about a dataframe, such as what it contains and the datatypes stored inside of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_dataframe.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also have pandas crunch bulk statistics on your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also perform excel functions/operations on data such as plotting using packages like matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pandas_dataframe.iloc[1:,0])\n",
    "print(type(pandas_dataframe.iloc[1,0])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe.iloc[1:,0] = pandas_dataframe.iloc[1:,0].astype(int)\n",
    "print(type(pandas_dataframe.iloc[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe.iloc[1:,0].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask - When to use DataFrames (from the Dask documentation)\n",
    "\n",
    "Dask DataFrames are used in situations where Pandas is commonly needed, but when Pandas is inadequare due to:\n",
    "\n",
    "* Manipulating large datasets, especially when those datasets don’t fit in memory\n",
    "* Accelerating long computations by using many cores\n",
    "* Distributed computing on large datasets with standard Pandas operations like groupby, join, and time series computations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe = dd.read_csv('Data/multiTimeline.csv')\n",
    "help(dd.read_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also convert directly from Pandas into Dask!\n",
    "* there is a low opportunity cost to using Pandas until your data gets too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe2 = dd.from_pandas(pandas_dataframe, npartitions=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe3 =  dd.from_pandas(pandas_dataframe, chunksize=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = dask_dataframe.loc['2019-07-07', 'Category: All categories']\n",
    "print(location.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_dataframe.loc['2019-07-07', 'Category: All categories']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dask_dataframe.loc['2019-07-07', 'Category: All categories'].compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dask_dataframe.columns.values)\n",
    "print(dask_dataframe.index.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_describe = dask_dataframe.describe()\n",
    "dask_describe.visualize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_describe.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrames Anti-Uses\n",
    "\n",
    "Dask DataFrame may not be the best choice in the following situations:\n",
    "\n",
    "* If your dataset fits into RAM on your laptop, just using Pandas. There are probably simpler ways to improve performance than  parallelism.\n",
    "* If your dataset doesn’t fit neatly into the Pandas tabular model, then you might find more use in dask.bag or dask.array.\n",
    "* If you need functions that are not implemented in Dask DataFrame, then you might want to look at dask.delayed which offers more flexibility.\n",
    "* If you need all of the features that databases offer you should consider PostgresSQL or MySQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
